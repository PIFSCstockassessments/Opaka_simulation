---
title: "Creating the OMs"
format: html
---

```{r}
library(r4ss)
library(tidyverse)
library(ss3sim)
library(stringr)
main.dir <- this.path::here(.. = 1)
source(file.path(main.dir, "R", "simF.r"))
source(file.path(main.dir, "R", "make_dummy_dat.r"))
source(file.path(main.dir, "R", "get_fits.r"))
set.seed <- read.csv(file.path(main.dir, "Inputs", "setseed.csv"))
sas_full <- read.csv(file.path(main.dir, "Inputs", "sas.csv"))
niter <- 100
rep <- SS_output(dir = file.path(main.dir, "models", "03-simplified-mod"))
recdevs_df <- read.csv(file.path(main.dir, "Inputs", "recdev_df.csv"))
F_df <- read.csv(file.path(main.dir, "Inputs" "F_df.csv"))
```

### Create rec dev timeseries

I generated a time series of recruitment deviations that result in a 40% decline in mean expected recruitment. To do this, I first calculated the expected recruitment based on the stock-recruitment curve for the first 75 years (solid yellow line). I found the average of that (dotted yellow line) and what a 40% decrease would be. From there, I created a vector of the rate of decline over 10 years to reach the 40% reduced level and then the reduced multiplier. I iteratively tested to find the true value of the decline multiplier that leads to a 40% decline in mean expected recruitment.

```{r}
recdevs_df <- rep$parameters %>% 
filter(str_detect(Label, "Rec")) %>% 
select(c(Label, Value)) %>% 
separate(col = Label, sep = "_", into = c("period", "recdev", "year")) 

ssb <- rep$derived_quants %>% 
filter(str_detect(Label, "SSB")) %>%
separate(col = Label, into = c("SSB", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year)) %>%
filter(!is.na(Year)) %>% 
pull(Value)

rec <- rep$derived_quants %>% 
filter(str_detect(Label, "Rec")) %>% 
separate(col = Label, into = c("Rec", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year)) %>%
filter(!is.na(Year)) %>% 
select(Year, Value) %>% 
rename("Rec" = "Value") %>% 
mutate("SSB" = ssb, 
"SR_rec" = (SSB*4*.76*180.859)/(1466.7*(1-.76)+SSB*(5*.76-1)),
"Calc_rec" = SR_rec*exp(c(0,recdevs_df$Value) - .657^2/2), 
"Rec_dev" = c(0, recdevs_df$Value))

#getting recruitment down to 40% reduction level
# 253.55 = last year of recruitment from the model ("rec$Rec")
#166.7759 is mean rec from model for last 5 years

decline_rate <- (0.65^(1/10)) - 1  # -4.2% per year for 35% total decline
years_decline <- 10
years_constant <- 15

# Create multiplier vector for decline years
decline_multiplier <- numeric(years_decline)
for(t in 1:years_decline) {
  decline_multiplier[t] <- (1 + decline_rate)^t
}

# Step 2: Create constant multiplier for remaining years
constant_multiplier <- rep(0.65, years_constant)  # 40% decline = 60% of original

# Step 3: Combine both periods
total_multiplier <- c(decline_multiplier, constant_multiplier)

# Step 4: Apply to your recent mean
recent_mean <- 162.4478  #166.7759mean from last 5 years #162.4478 is mean SR-curve rec
extended_dec_vec <- recent_mean * total_multiplier

# Step 5: Calculate recruitment deviations for all 25 years
sigma_r <- .35
extended_rec_devs <- log(extended_dec_vec / recent_mean) - ((sigma_r^2)/2)
plot(x = 1:length(extended_rec_devs), y = extended_rec_devs)

omctl <- SS_readctl_3.30(file = file.path("models", "opaka-om-25-recdevs-test", "codOM.ctl"),
datlist = file.path("models", "opaka-om-25-recdevs-test", "codOM.dat"))
omctl$recdev_input[76:100,2] <- extended_rec_devs
omctl$F_setup2[76:100,"Fvalue"] <- F_comm_df[76:100,1]
omctl$F_setup2[176:200,"Fvalue"] <- F_noncomm_df[76:100,1]
# omctl$F_setup2[76:100,"Fvalue"] <- 0
# omctl$F_setup2[176:200,"Fvalue"] <- 0

SS_writectl_3.30(omctl, outfile = file.path("models", "opaka-om-25-recdevs-test", "codOM.ctl"), overwrite = T)

#run ss3 -nohess for om

reptest <- SS_output(file.path("models", "opaka-om-25-recdevs-test"))

reptest$derived_quants %>% filter(str_detect(Label, "Rec")) %>%
separate(col = Label, into = c("Recr", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year),
    period = ifelse(Year <=75 & Year >=69, "early", "late"),
    period = ifelse(Year <69, "NA", period),
    period = ifelse(Year >75 & Year <=85, "decline", period),
    period = ifelse(is.na(Year), "init", period)) %>%
    group_by(period) %>% summarise(mean = mean(Value), min = min(Value), max = max(Value))

# # Adjust decline multiplier
# if(actual_decline < target_decline) {
#     decline_multiplier <- decline_multiplier * 1.1
# } else {
#     decline_multiplier <- decline_multiplier * 0.9
# }

reptest_rec <- reptest$derived_quants %>% filter(str_detect(Label, "Rec")) %>%
separate(col = Label, into = c("Recr", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year)) %>% filter(!is.na(Year))

plot(rec$Year, y = rec$SR_rec, type = "l", col = "orange", xlim = c(1,100), ylim = c(0,800))
abline(h = 187.0477, col = "darkblue", lty = 2) #mean Rec
abline(h = 167.6124, col = "darkorange", lty = 2) #mean SR recruitment
abline(h = 100.5674, col = "red4", lty = 2) #reduced by 40% mean SR recruitment
lines(rec$Year, y = rec$Rec, col = "blue")
lines(rec$Year, y = rec$Calc_rec, col = "blue", lty = 2)
lines(76:100, y = extended_dec_vec, col = "red", lty = 3, lwd = 1.5)
#lines(86:100, y = extended_dec_vec, col = "red", lty = 3, lwd = 1.5)
lines(1:100, y = reptest_rec$Value, col = "red")

```

#### Rec dev matrix

Create 100 rec dev trajectories for 75 years, with added variability (sigmaR = .15) in the last 10 years. The first 65 years will follow the recruitment deviations that came out of the assessment model to retain any sort of environmental signal it may have picked up on.

```{r}
set.seed(1234)
plot(x = recdevs_df$year, y = recdevs_df$Value, type = "l")
sigmar <- .15 #.657 from opaka model
new_recdevs <- matrix(data = NA, nrow = nrow(recdevs_df)+1, ncol = 100)
recdevs <- matrix(data = NA, nrow = nrow(recdevs_df)+1, ncol = 100)
new_recdevs[1,] <- 0
for(i in 2:nrow(new_recdevs)){
    if(i < 60){
        sample_sd <- .01
        # new_recdevs[i,] <- faux::rnorm_multi(n = 100, ## year vector
        #                          vars = 1,
        #                          mu = recdevs_df$Value[i-1],
        #                          r = 0.5, #correlation
        #                          sd = sample_sd)[,1]
        
        new_recdevs[i,] <- rnorm(n = 100, mean = recdevs_df$Value[i-1], sd = .01)  
        # new_recdevs[i,] <- sigmar * recdevs[i,] - sigmar^2 / 2


    }else{
        recdevs[i,] <- rnorm(n = 100, mean = 0, sd = 1)  
        new_recdevs[i,] <- sigmar * recdevs[i,] - sigmar^2 / 2 
    }


}
matplot(new_recdevs, type = "l")
```

Then we add 25 years of assumed "normal" future recruitment, with the same variaiblity as the last 10 years of the historical recdevs (sigmaR = 0.15).

```{r}
future_recdevs <- matrix(data = NA, nrow = 50, ncol = 100)
for(i in 1:nrow(future_recdevs)){ 
    recdevs <- rnorm(n = 100, mean = 0, sd = 1)   
    future_recdevs[i,] <- sigmar * recdevs - sigmar^2 / 2 
}
matplot(y = future_recdevs, type = "l")

full_recdevs <- rbind(new_recdevs, future_recdevs)
matplot(y = full_recdevs, type = "l")

# save(list = "full_recdevs", file = file.path(main.dir, "Inputs", "recdevs_mat.RData"))
```

We also create 25 years of "poor" future recruitment. Using the full_recdevs, 100 OM and EMs were generated and the results were aggregated using the `get_results_all` function for that scenario (read in as `lrf` in code chunk below). The recruitment in year 75 for each iteration (should be the same across scenarios for the same iteration) was taken and then a trajectory was created that brought that value down to the calculated 40% reduced mean recruitment. Those values were randomly sampled with a sd of `sigmaR` to add noise and those recdevs were added to the `new_recdevs` to create a second matrix, with poor recruitment values.  

```{r}
# Empty matrix to store rec devs
poor_recdevs <- matrix(NA, nrow = length(total_multiplier), ncol = 100)

# Base recruitment deviations (your original calculation)
base_rec_devs <- log(extended_dec_vec / recent_mean) - (sigma_r^2)/2

for(i in 1:100) {
  # Add random variation to recruitment deviations
  random_devs <- rnorm(length(total_multiplier), mean = 0, sd = .15)
  poor_recdevs[, i] <- base_rec_devs + random_devs
  
}

full_poor_recdevs <- rbind(new_recdevs, poor_recdevs)
matplot(y = full_poor_recdevs, type = "l")
matplot(y = full_recdevs, type = "l", add = T)
abline(h = 0)
abline(v = 75)

# save(list = "full_poor_recdevs", file = file.path(main.dir, "Inputs", "poor_recdevs_mat.RData"))
```

## Simulating F trajectory
We simulated 100 stochastic F time series, based on the true time series of F for the commercial and non commercial fleets estimated in the original Opakapaka assessment model. For the 25 years in the future, we used an average F value from the terminal 5 years. F values also had fairly strong temporal autocorrelation (0.7) which makes sense because management is consistent over 3-5 years of time and you would not expect fishing pressure to change drastically from year to year randomly. For non-commercial F, after year 2004, we calculated it as a ratio of 1.03 to commercial F, as is done in the most recent assessment. 
```{r}

# Get the mean F of last 5 years from Opaka assessment
mean_F <- F_df %>% 
  slice((n()-4):n()) %>%
  summarise(mean_FRS = mean(FRS), 
            mean_Non_comm = mean(Non_comm))

FRS_mu_ts <- c(F_df$FRS[-1], rep(mean_F$mean_FRS, 25))
NC_mu_tcs <- c(F_df$Non_comm[-1], rep(mean_F$mean_Non_comm, 25))

n_sims <- 100
temporal_cor <- 0.7
# Function to calculate temporally correlated F values based on a time series of "true" F values 
calculate_F <- function(n_sims, n_years, mu_ts, temporal_cor){
  F_df <- matrix(data = NA, nrow = n_years, ncol = n_sims)
  for(sim in 1:n_sims) {
      # Initialize first year with bounds
      F_df[1, sim] <- pmax(0.001, pmin(0.08, rnorm(1, mean = mu_ts[1], sd = 0.002)))
      
      # Generate subsequent years with AR(1) correlation
      for(year in 2:n_years) {
        # AR(1): F_t = rho * (F_{t-1} - mu_{t-1}) + mu_t + epsilon_t
        temp_cor <- rnorm(1, 0, 0.002 * sqrt(1 - temporal_cor^2))
        F_candidate <- temporal_cor * (F_df[year-1, sim] - mu_ts[year-1]) + 
                                mu_ts[year] + temp_cor
        F_df[year, sim] <- pmax(0.001, pmin(0.08, F_candidate))
      }
  }
  return(F_df)
}

F_comm_df <- calculate_F(n_sims = 100, n_years = 100, mu_ts = FRS_mu_ts, temporal_cor)
matplot(y = F_comm_df, type = "l", lty = 1, lwd = 1.2)

F_noncomm_df <- calculate_F(n_sims = 100, n_years = 55, mu_ts = NC_mu_ts[1:75], temporal_cor)
F_noncomm_df <- rbind(F_noncomm_df, (F_comm_df[56:100,]*1.03))
matplot(y = F_noncomm_df, type = "l", lty = 1, lwd = 1.2)

plot(x = 1:100, y = F_comm_df[,1], type = "l")
lines(x = 1:100, y = F_noncomm_df[,1], col = "red")
#simpler way of generating F's keeping first 75 years exact same as opaka model, then random based on terminal year F (future_cf code removed).
#does not include any correlation between years or series
# F_comm_mod <- rep$exploitation$Comm[-1]
#F_comm_df <- matrix(data = rep(F_comm_mod, 100), nrow = 75, ncol = 100,byrow = F)
#F_comm_df <- rbind(F_comm_df, future_cf)
# F_noncomm_mod <- rep$exploitation$Non_comm[-1]
#F_noncomm_df <- matrix(data = rep(F_noncomm_mod, 100), nrow = 75, ncol = 100,byrow = F)
#F_noncomm_df <- rbind(F_noncomm_df, future_cf * 1.03)

# Save output 
# save(list = c("F_noncomm_df", "F_comm_df"), file = file.path(main.dir, "Inputs", "constantF_mat.RData"))

```

### Load already created RecDev and F vectors

```{r}
load(file.path(main.dir, "Inputs", "recdevs_mat.RData"))
load(file.path(main.dir, "Inputs", "poor_recdevs_mat.RData"))
load(file.path(main.dir, "Inputs", "constantF_mat.RData"))
```

### Create the directories for OSG and target_dir.txt

```{r}

library(fs)

scen <- "HRF_SQ"
nyears_fwd <- 25
niter <- 100
scen_dirs <- file.path(main.dir, paste(scen, nyears_fwd, "yrfwd", sep = "_"), 1:niter)
sapply(scen_dirs, dir.create, recursive = T)
# file.copy(from = file.path(main.dir, "Inputs", "recdevs_mat.RData"), file.path(scen_dirs, "recdevs_mat.RData"))
# file.copy(from = file.path(main.dir, "Inputs", "poor_recdevs_mat.RData"), file.path(scen_dirs, "poor_recdevs_mat.RData"))
file.copy(from = file.path(main.dir, "Inputs", "setseed.csv"), file.path(scen_dirs, "setseed.csv"))
file.copy(from = file.path(main.dir, "Inputs", "sas.csv"), file.path(scen_dirs, "sas.csv"))
file.copy(from = file.path(main.dir, "Inputs", "F_df.csv"), file.path(scen_dirs, "F_df.csv"))
file.copy(from = file.path(main.dir, "Inputs", "recdev_df.csv"), file.path(scen_dirs, "recdev_df.csv"))
file.copy(from = file.path(main.dir, "Inputs", "effN.csv"), file.path(scen_dirs, "effN.csv"))
# file.copy(from = file.path(main.dir, "Inputs", "constantF_mat.RData"), file.path(scen_dirs, "constantF_mat.RData"))

for(i in 1:length(scen_dirs)){
    fs::dir_copy(file.path(main.dir, "models", paste0("opaka-om", nyears_fwd)), scen_dirs[i]) #"-FRS-only" #, "-selex" , "-R0_trend"
    fs::dir_copy(file.path(main.dir, "models", paste0("opaka-em", nyears_fwd)), scen_dirs[i])
}
# only for FRS_only_poorrec_25_yrfwd scenario (testing hyperstablity)
# for(i in 1:length(scen_dirs)){
#     file.copy(from = file.path(main.dir, "FRS_only_25_yrfwd", i, "em", "ss3.dat"), 
#     to = file.path(scen_dirs[i], "normal_rec.dat"))
# }

for(i in 1:length(scen_dirs)){
    
    shell(paste0("powershell cd ", scen_dirs[i], ";tar -czf start.tar.gz ./*"))
    files_clean <- list.files(scen_dirs[i], pattern = ".csv|.RData|opaka-|normal_rec.dat", full.names = T)
    unlink(files_clean, recursive = T)
}

target_dirs <- paste0(scen, "/", 1:niter)
writeLines(target_dirs, file.path(main.dir, scen, "target_dirs.txt"))
shell(paste0("powershell cd ", main.dir, ";tar -czf upload.", scen, "_.tar.gz ", scen))
#upload these files to OSG 
```

### Getting results

```{r}
#get results out of models for comparisons
all_scenario_names <- paste(sas_full$Scen_name, sas_full$N_years, "yrfwd", sep = "_")[c(1:8,10)]

missing_vec <- c()
for(i in 1:length(all_scenario_names)){
    if(file.exists(file.path(main.dir, paste0("download.", all_scenario_names[i], ".tar.gz ")))){
            shell(paste0("powershell cd ", main.dir, ";tar -xzf download.", all_scenario_names[i], ".tar.gz ", all_scenario_names[i]))
            for(y in 1:niter){
                if(file.exists(file.path(main.dir, all_scenario_names[i], y, "End.tar.gz"))){
                    shell(paste0("powershell cd ", main.dir, "/", all_scenario_names[i], "/", y, ";tar -xzf End.tar.gz"))
                }else{
                    missing_vec <- c(missing_vec, paste0(all_scenario_names[i], y))
                } 
            }
    }else{
        next
    }

}

get_results_all(
    overwrite_files = T,
    user_scenarios =  all_scenario_names[i],
    filename_prefix = "hrf_sq"
)   
get_fits_all(
    overwrite_files = T,
    user_scenarios = all_scenario_names[i],
    filename_prefix = "hrf_sq"
)

```

### Checking results

```{r}

reps <- SSgetoutput(dirvec = c(
# file.path(main.dir, "HRF_SQ_25_yrfwd", "10", "om"),
# file.path(main.dir, "HRF_SQ_25_yrfwd", "10", "em"),
# file.path(main.dir, "HRF_SQ_25_yrfwd", "20", "om"),
# file.path(main.dir, "HRF_SQ_25_yrfwd", "20", "em"),
file.path(main.dir, "HRF_SQ", "1", "om"),
file.path(main.dir, "HRF_SQ", "1", "em"), 
file.path(main.dir, "HRF_SQ", "2", "om"),
file.path(main.dir, "HRF_SQ", "2", "em"), 
file.path(main.dir, "HRF_SQ", "3", "om"),
file.path(main.dir, "HRF_SQ", "3", "em"), 
file.path(main.dir, "HRF_SQ", "4", "om"),
file.path(main.dir, "HRF_SQ", "4", "em"), 
file.path(main.dir, "HRF_SQ", "5", "om"),
file.path(main.dir, "HRF_SQ", "5", "em") 
))

reps_sum <- SSsummarize(reps)
SSplotComparisons(reps_sum, col = c(1,2))
SSplotComparisons(reps_sum, legendlabels = c("prOM", "prEM"),  models = c(3,4), uncertainty = F)
SS_plots(reps$replist2)
SSplotRecdevs(reps$replist2)
bias <- SS_fitbiasramp(reps$replist2)

rep <- SS_output(file.path(main.dir, "models", "opaka-em-25"))
SS_plots(rep)

reps_sum$SpawnBio %>% mutate(re = (replist2-replist1)/replist1) %>% filter(Yr > 0) %>% ggplot(aes(x = Yr, y = re)) + geom_line() + geom_hline(yintercept = 0)

reps_sum$Fvalue %>% mutate(re = (replist2-replist1)/replist1) %>% filter(Yr > 0)%>% summarise(min = min(re), max = max(re))
reps_sum$recruits %>% mutate(re = (replist2-replist1)/replist1) %>% filter(Yr > 0) %>% summarise(min = min(re), max = max(re))

ssb_re_original <- ts_re %>% 
filter(scenario == "HRF_SQ_25_yrfwd" & iteration <= 10) %>% 
select(year, SpawnBio_re, iteration, scenario) %>% 
filter(year <= 100) %>% 
group_by(year) %>% 
summarise(mean = mean(SpawnBio_re))

ssb_re_new <- ts_re %>% 
filter(scenario == "HRF_SQ_25_yrfwd" & iteration <= 10) %>% 
select(year, SpawnBio_re, iteration, scenario) %>% 
filter(year <= 100) %>% 
group_by(year) %>% 
summarise(mean = mean(SpawnBio_re))

ssb_re_new_100 <- ts_re %>% 
#filter(scenario == "HRF_SQ_25_yrfwd") %>% 
select(year, SpawnBio_re, iteration, scenario) %>% 
filter(year <= 2023) %>% 
group_by(year) %>% 
summarise(mean = mean(SpawnBio_re),
median = quantile(SpawnBio_re, .5))


# Filter for the last year and calculate cumulative mean
cumulative_means <- ts_re %>%
  filter(year == 100) %>%
  arrange(iteration) %>%  # Make sure iterations are in order
  mutate(
    cumulative_mean = cumsum(SpawnBio_re) / row_number(),
    n_iterations = row_number()
  )

# Plot the results
ggplot(cumulative_means, aes(x = n_iterations, y = cumulative_mean)) +
  geom_line(size = 1) +
  geom_point(alpha = 0.6) +
  labs(
    x = "Number of Iterations",
    y = "Mean SpawnBio_re",
    title = "Convergence of Mean SpawnBio_re with Increasing Iterations",
    subtitle = "Year 100"
  ) +
  theme_minimal()

ssb_re_new_100 %>% ggplot(aes(x = year, y = mean)) + geom_line() + geom_hline(yintercept = 0) +
#geom_line(data = ssb_re_new, aes(x = year, y = mean), color = "blue") + 
geom_line(data = ssb_re_new_100, aes(x = year, y = mean), color = "red") +
geom_line(data = ssb_re_new_100, aes(x = year, y = median), color = "blue") 

colnames(reps_sum$SpawnBio) <- c("om_1", "em_1", "om_2", "em_2", "om_3", "em_3", "om_4", "em_4","om_5", "em_5", "label", "year")

results_ts %>% select(SpawnBio, year, model_run, iteration) %>% 
pivot_wider(names_from = model_run, values_from = SpawnBio) %>% 
mutate(re = (em-om)/om) %>%
  filter(year > 0 & year < 101) %>%
  group_by(year) %>%
  summarise(mean = mean(re), 
  uci = quantile(re, .975), lci = quantile(re, .05))


sim_re <- ts_re %>% filter(year > 0 & year < 101) %>% group_by(year) %>%
  summarise(mean = mean(SpawnBio_re), 
  uci = quantile(SpawnBio_re, .975), lci = quantile(SpawnBio_re, .05))

reps_sum$SpawnBio %>%
pivot_longer(
    cols = matches("^(om|em)_\\d+$"),  # only columns matching the pattern
    names_to = c("method", "number"),
    names_pattern = "(.+)_(.+)",
    values_to = "value"
  ) %>%
  pivot_wider(
    names_from = method,
    values_from = value
  ) %>% 
  mutate(re = (em-om)/om) %>%
  filter(year > 0 & year < 101) %>%
  group_by(year) %>%
  summarise(mean = mean(re), 
  uci = quantile(re, .975), lci = quantile(re, .05)) %>% 
  ggplot(aes(x = year)) + 
  geom_line(aes(y = mean)) + 
  geom_hline(yintercept = 0) 
  geom_line(data = sim_re, aes(x = year, y = mean), color = "blue")

reps$replist1$derived_quants %>% filter(str_detect(Label, "Rec")) %>%
separate(col = Label, into = c("Recr", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year),
    period = ifelse(Year <=75, "early", "late"),
    period = ifelse(Year >75 & Year <=85, "decline", period),
    period = ifelse(is.na(Year), "init", period)) %>%
    group_by(period) %>% summarise(mean = mean(Value), min = min(Value), max = max(Value))

rep <- SS_output(file.path(main.dir, "HRF_poorrec_perfectdata_25_yrfwd/9/em"))
SS_plots(rep)

ssb_msy <- rep$derived_quants %>% filter(str_detect(Label, "SSB_MSY")) %>% pull(Value) 
.865*ssb_msy
rep$exploitation


reps_sum <- SSsummarize(reps)
#SSplotComparisons(reps_sum, legendlabels = c("HRF_om", "HRF_em", "LRF_om", "LRF_em"))  print = TRUE, png = TRUE, plotdir = file.path(main.dir, "HRF_test", "2")
SSplotComparisons(reps_sum, legendlabels = c("om1", "em1", "om10", "em10"))
SS_plots(reps$replist1)
SS_plots(reps$replist2)
SS_plots(reps$replist3)

reps_sum$maxgrad
reps$replist2$breakpoints_for_bias_adjustment_ramp
new_bias <- SS_fitbiasramp(reps$replist4)

reps_sum$indices %>% filter(Fleet == 3) %>% #filter(name == "replist1" | name == "replist2") %>%
mutate(name = recode(name, 
                    replist1 = "om_1", 
                    replist2 = "em_1", 
                    replist3 = "om_2",
                    replist4 = "em_2",
                    replist5 = "om_3",
                    replist6 = "em_3",
                    replist7 = "om_4",
                    replist8 = "em_4",
                    replist9 = "om_5",
                    replist10 = "em_5"
                    )) %>%
separate(name, sep = "_", into = c("mod_type", "rep")) %>%
ggplot(aes(x = Yr)) +
geom_point(data = . %>% filter(mod_type == "om"), aes(y = Exp, color = mod_type)) + 
geom_point(data = . %>% filter(mod_type == "em"), aes(y = Obs, color = mod_type)) + 
geom_line(data = . %>% filter(mod_type == "em"), aes(y = Exp, color = mod_type), linewidth = 1.5) +
facet_wrap(~rep) +
labs(x = "Year", y = "BFISH Index") + 
theme_classic() +
theme(legend.position = "top",
legend.text = element_text(size = 12))

```

Questions from Punt et al. 2021 to think about for this project: 1. How does the ability to estimate quantities of management interest change with average annual (effective) sample sizes for length-comp and conditional age-length data? 2. Does including data sources with small sample sizes in an assessment lead to bias? 3. What are the implications of collecting length and age samples at intervals greater than one year but large sample sizes? 4. How does the assessment performance change if sampling is unbalanced over time and space? Questions from Ono et al. 2015 1. What is the value of age vs length composition data? 2. What is the influence of age composition collection frequency and duration? 3. What is the influence of frequency and duration of survey composition data? 4. What is the impact of historical fishery composition data? 5. What is the impact of composition data sample size? 6. Which fishing pattern produces the most informative data?

### Run likelihood profiles

To profile over an F value for a given year, use the following code:

```{r}
em_I <- "SQ_0_yrfwd"
Iteration <- 5
nyears <- 75
like_dir <- file.path(main.dir, em_I, Iteration, "em", "likelihood_prof")
dir.create(like_dir)
files <- c("ss3.dat", "control.ss_new", "starter.ss", "forecast.ss")
file.copy(from = file.path(main.dir, em_I, Iteration,"em", files), to = like_dir, overwrite = T)
file.copy(from = file.path(main.dir, "bin", "ss3_win.exe"), to = like_dir, overwrite = T)
## Create fvec of values to profile over
rep_tmp <- SS_output(file.path(main.dir, em_I, Iteration, "em"), verbose = F)
fest <- rep_tmp$exploitation %>% filter(Yr == nyears) %>% pull(Comm)
fvec <- seq(fest - 0.01, fest + 0.02, by = 0.001)
fdf <- rep_tmp$exploitation %>% 
select(Yr, Seas, Comm, Non_comm) %>%
pivot_longer(cols = c("Comm", "Non_comm")) %>% 
mutate(Fleet = recode(name, Comm = "1", Non_comm = "2"),
Fleet = as.numeric(Fleet),
se = 0.5,
phase = -1) %>%
filter(Yr > 0) %>%
arrange(Fleet, Yr) %>% 
select(Fleet, Yr, Seas, value, se, phase) 

## Fix selex first 
CTL <- SS_readctl_3.30(file.path(like_dir, "control.ss_new"), datlist = file.path(like_dir, "ss3.dat"))
CTL$size_selex_parms$PHASE <- abs(CTL$size_selex_parms$PHASE) * -1
SS_writectl_3.30(CTL, outfile = file.path(like_dir, "control.ss_new"), overwrite = T)
rm(CTL)
## read in ctl file
CTL <- readLines(file.path(like_dir, "control.ss_new"))
# selex_lines <- grep("#_SizeSelex", CTL)
# end_selex_lines <- grep("#_AgeSelex", CTL)
# selex_params <- CTL[(selex_lines+2):(end_selex_lines-1)]
# selex <- strsplit(selex_params, " ")
# selex_new <- list()
# for(i in 1:length(selex)){
#     ind <- which(selex[[i]] == 3)
#     if(length(ind) == 1){
#         selex[[i]][ind] <- "-3"
#     }
#     selex[[i]] <- selex[[i]][-which(selex[[i]] == "")]
#     selex_new[[i]] <- str_flatten(selex[[i]], collapse =  " ")
# }
# ## replace selex values
# CTL[237] <- selex_new[[1]]
# CTL[238] <- selex_new[[2]]
# CTL[239] <- selex_new[[3]]
# CTL[240] <- selex_new[[4]]
# CTL[241] <- selex_new[[5]]
# CTL[242] <- selex_new[[6]]

## find lines where to break the file and insert new lines
start_ind <- grep("#Fishing Mortality info", CTL)
end_ind <- grep("#_initial_F_parms", CTL)

f_setup <- paste0("## add new fishing mortality info \n",
"#Fishing Mortality info \n",
"0.3 # F ballpark value in units of annual_F \n",
"-2001 # F ballpark year (neg value to disable) \n",
"4 # F_Method:  1=Pope midseason rate; 2=F as parameter; 3=F as hybrid; 4=fleet-specific parm/hybrid (#4 is superset of #2 and #3 and is recommended) \n",
"2.9 # max F (methods 2-4) or harvest fraction (method 1) \n",
"#fleet parameter_value phase \n",
"1 0.015 1 \n",
"2 0.015 1 \n",
"-9998 1 1 \n",
"3 #Ntuning loops \n",
"# detailed setup for F_Method=2; -Yr to fill remaining years \n",
"#Fleet Yr Seas F_value se phase \n")

f_detailed_ender <- " -9999 1 1 1 1 1 \n"


## write out new ctl file
writeLines(CTL[1:(start_ind -1)], file.path(like_dir, "control_modified.ss"))
CON <- file(file.path(like_dir, "control_modified.ss"), "a")
cat(f_setup, file = CON)
write.table(fdf, file = CON, append = T, col.names = F, row.names = F)
cat(f_detailed_ender, file = CON) 
writeLines(CTL[end_ind:length(CTL)], CON)
close(CON)

#adjust se for catch 
dat <- SS_readdat_3.30(file = file.path(like_dir, "ss3.dat"), verbose = F)
dat$catch$catch_se <- .5
SS_writedat(dat, outfile = file.path(like_dir, "ss3.dat"), overwrite = T)
## Create profile directories
profile_dirs <- paste0(like_dir, "/profile_", seq(1,length(fvec)))
sapply(profile_dirs, dir.create)
profile.files <- c("ss3.dat", "control_modified.ss", "starter.ss", "forecast.ss", "ss3_win.exe")
for (i in 1:length(fvec)){
    file.copy(from = file.path(like_dir, profile.files), to = profile_dirs[i], overwrite = T)

    CTL <- readLines(file.path(profile_dirs[i], "control_modified.ss"))
    line_ind <- grep(paste0("1 ",nyears, " 1"), CTL)
    CTL[line_ind] <- paste0("1 ", nyears, " 1 ", fvec[i], " 0.5 -1")
    writeLines(CTL, file.path(profile_dirs[i], "control_modified.ss"))

    START <- SS_readstarter(file.path(profile_dirs[i], "starter.ss"))
    START$ctlfile <- "control_modified.ss"
    START$prior_like <- 1
    START$init_values_src <- 0
    SS_writestarter(START, profile_dirs[i], overwrite = T)

    run(dir = profile_dirs[i], exe = "ss3_win.exe", extras = "-nohess", skipfinished = F, verbose = F)

}



```

```{r}
#em_I <- paste0("em_", n_yrs_fwd)
em_I <- "HRFhyperstable_15_yrfwd"
Iteration <- 1
like_dir <- file.path(main.dir, em_I, Iteration, "em", "likelihood_prof")
dir.create(like_dir)
files <- c("ss3.dat", "control.ss_new", "starter.ss", "forecast.ss", "ss.par", "ss3.exe")
file.copy(from = file.path(main.dir, em_I, Iteration,"em", files), to = like_dir, overwrite = T)
CTL <- SS_readctl_3.30(file = file.path(like_dir, "control.ss_new"), datlist = file.path(like_dir, "ss3.dat"))
CTL <- SS_readctl_3.30(file = file.path(main.dir, em_I, Iteration, "em", "control.ss_new"), datlist = file.path(main.dir, em_I, Iteration, "em", "ss3.dat"))
# r0 <- CTL$SR_parms$INIT[1]
# r0_vec <- seq(r0 - 1, r0 + 1, by = 0.2)
CTL$F_setup2

par.file <- SS_readpar_3.30(parfile = file.path(like_dir, "ss.par"), datsource = file.path(like_dir, "ss3.dat"), ctlsource = file.path(like_dir, "control.ss_new"))
F_term <- par.file$F_rate %>% filter(fleet == 1 & year == nyears) %>% pull(F)
F_term_vec <- seq(F_term - .01, F_term + 0.1, by = 0.005)

START <- SS_readstarter(file = file.path(like_dir, "starter.ss"), verbose = FALSE)
START$prior_like <- 1
START$ctlfile <- "control_modified.ss"
START$init_values_src <- 1
SS_writestarter(START, dir = like_dir, overwrite = TRUE, verbose = F)

likes <- profile_F(
  dir = like_dir,
  newctlfile = "control_modified.ss",
  linenum = nyears,
  #string = "Fvalue",
  profilevec = F_term_vec[10:20],
  usepar = T,
  parstring = "# F_rate[90]:",
  exe = "ss3",
  verbose = FALSE,
  extras="-nohess -nox"
)



likes <- profile(
  dir = like_dir,
  newctlfile = "control_modified.ss",
  linenum = 241,
  #string = "Fvalue",
  profilevec = F_term_vec,
  usepar = T,
  parstring = "# F_rate[90]:",
  exe = "ss3",
  verbose = FALSE,
  extras="-nohess -nox"
)

profile_mods <- SSgetoutput(dirvec = like_dir, keyvec = c(1:5,7,10))
profile_mods_sum <- SSsummarize(profile_mods)
profile_mods_sum$maxgrad <= 1e-4

SSplotProfile(profile_mods_sum,
  profile.string = "SR_LN",
  profile.label = "SR_LN(R0)",
  print = TRUE,
  plotdir = like_dir,

)
PinerPlot(profile_mods_sum, component = "Length_like", print = TRUE, plotdir = like_dir)
PinerPlot(profile_mods_sum, component = "Surv_like")




```