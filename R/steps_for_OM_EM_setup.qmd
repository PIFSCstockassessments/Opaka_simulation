---
title: "Creating the OMs"
format: html
---

```{r}
library(r4ss)
library(tidyverse)
library(ss3sim)
library(stringr)

main.dir <- this.path::here(.. = 1)
source(file.path(main.dir, "R", "get_fits.r"))
set.seed <- read.csv(file.path(main.dir, "Inputs", "setseed.csv"))
sas_full <- read.csv(file.path(main.dir, "Inputs", "sas.csv"))
niter <- 200
#recdevs_df <- read.csv(file.path(main.dir, "Inputs", "recdev_df.csv"))
F_df <- read.csv(file.path(main.dir, "Inputs", "F_df.csv"))
load("Inputs/constantF_mat.Rdata")
```

## Recreating the recruitment calculations from SS
```{r}
#| eval: false
#| echo: false

rep <- SS_output(file.path(main.dir, "IRF_SQ_25_yrfwd", "1", "om"))

recdevs_df <- rep$recruit %>% 
select(Yr, SpawnBio, exp_recr, dev, biasadjuster)

ss_rec <- rep$derived_quants %>% 
filter(str_detect(Label, "Recr")) %>%
separate(Label, into = c("rec", "Yr"), sep = "_") %>% 
mutate(Yr = as.numeric(Yr), "SS_rec" = Value) %>% 
filter(!is.na(Yr)) %>% 
select(Yr, SS_rec)

rec <- recdevs_df %>% 
mutate("Calc_rec" = exp_recr * exp(-.5*biasadjuster*(.52^2)+dev)) %>% 
left_join(ss_rec, by = "Yr")  #want SS_rec and Cacl_rec values to match up

```


### Getting the "good" iterations for declining recruitment scenarios
For the SQ recruitment scenarios, recdevs were randomly generated by ss3sim. Out of 800 runs (200 iterations per scenario), 33 did not converge. To keep the results comparable across recruitment scenarios, we selected 100 runs that led to convergence for the SQ models. 

```{r}
its <- seq(1, 200)
good_its <- setdiff(its, unique(nonconverged_its$iteration))
good_its_100 <- good_its[1:100]
```

### Create rec dev timeseries

I generated a time series of recruitment deviations that result in a 40% decline in mean expected recruitment. To do this, I first calculated the expected recruitment based on the stock-recruitment curve for 100 years. I found the average of that and what a 40% decrease would be. From there, I created a vector of the rate of decline over 10 years to reach the 40% reduced level and then the reduced multiplier. I iteratively tested to find the true value of the decline multiplier that leads to a 40% decline in mean expected recruitment.

```{r}

#getting recruitment down to 40% reduction level
# 283.9170 = last year of recruitment from the model ("rec$Rec")
# rec %>% filter(Yr > 2035) %>% summarise(mean(SS_rec)) = 234.168 is mean rec from model for last 15 years
# rec %>% filter(Yr > 2035) %>% summarise(mean(exp_recr)) = 262.9344

decline_rate <- (0.71^(1/10)) - 1  # 30% to account for effect of lower SSB
years_decline <- 10
years_constant <- 15

# Create multiplier vector for decline years
decline_multiplier <- numeric(years_decline)
for(t in 1:years_decline) {
  decline_multiplier[t] <- (1 + decline_rate)^t
}

# Step 2: Create constant multiplier for remaining years
constant_multiplier <- rep(0.71, years_constant)  # 40% decline = 60% of original (only 30% to account for effect of reduced SSB and SR relationship not linear)

# Step 3: Combine both periods
total_multiplier <- c(decline_multiplier, constant_multiplier)

# Step 4: Apply to your recent mean
recent_mean <- 279.67   #279.67 mean from last 15 years #165.2207 is mean 
extended_dec_vec <- recent_mean * total_multiplier

# Step 5: Calculate recruitment deviations for all 25 years
sigma_r <- .52
extended_rec_devs <- log(extended_dec_vec / recent_mean) - ((sigma_r^2)/2)
plot(x = 1:length(extended_rec_devs), y = extended_rec_devs)

omctl <- SS_readctl_3.30(file = file.path("models", "opaka-om-25-recdevs-test", "codOM.ctl"),
datlist = file.path("models", "opaka-om-25-recdevs-test", "codOM.dat"))
omctl$N_Read_recdevs <- 100
omctl$recdev_input <- data.frame(yr = 1949:2048, c(recdevs_df$dev[1:75], extended_rec_devs))
omctl$F_setup[3] <- 200
omctl$F_setup2 <- data.frame(fleet = 1, yr = 1949:2048, 
                              seas = 1, Fvalue = F_comm_df[,1], 
                              se = 0.01, phase = -1)

omctl$F_setup2 <- rbind(omctl$F_setup2, 
                        data.frame(fleet = 2, yr = 1949:2048, 
                              seas = 1, Fvalue = F_noncomm_df[,1], 
                              se = 0.01, phase = -1))


SS_writectl_3.30(omctl, outfile = file.path("models", "opaka-om-25-recdevs-test", "codOM.ctl"), overwrite = T)

#run ss3 -nohess for om

reptest <- SS_output(file.path("models", "opaka-om-25-recdevs-test"), covar = F)
#SS_plots(reptest)
## looking for ~167 in rec 

reps <- SSgetoutput(dirvec = c(file.path("models", "opaka-om-25-recdevs-test"), 
file.path(main.dir, "HRF_SQ_25_yrfwd", "1", "em")))
repssum <- SSsummarize(reps)
SSplotComparisons(repssum)

reptest$derived_quants %>% filter(str_detect(Label, "Rec")) %>%
separate(col = Label, into = c("Recr", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year),
    period = ifelse(Year <= 2023, "main", "decline"),
    period = ifelse(Year > 2033, "low", period)) %>%
    group_by(period) %>% summarise(mean = mean(Value), min = min(Value), max = max(Value))

# # Adjust decline multiplier
# if(actual_decline < target_decline) {
#     decline_multiplier <- decline_multiplier * 1.1
# } else {
#     decline_multiplier <- decline_multiplier * 0.9
# }


```

#### Rec dev matrix

We create 25 years of "poor" future recruitment. Using the full_recdevs, 100 OM and EMs were generated and the results were aggregated using the `get_results_all` function for that scenario (read in as `lrf` in code chunk below). The recruitment in year 75 for each iteration (should be the same across scenarios for the same iteration) was taken and then a trajectory was created that brought that value down to the calculated 40% reduced mean recruitment. Those values were randomly sampled with a sd of `sigmaR` to add noise and those recdevs were added to the `new_recdevs` to create a second matrix, with poor recruitment values.  

```{r}

sigma_r <- .52
# Empty matrix to store rec devs
poor_recdevs <- matrix(0, nrow = length(total_multiplier), ncol = 200)

sq_recdevs <- ts_df %>% select(year, rec_dev, model_run, iteration, scenario) %>% 
filter(model_run == "om" & scenario == "IRF_SQ_25_yrfwd") %>%
pivot_wider(values_from = rec_dev, names_from = iteration) 


# Base recruitment deviations (your original calculation)
base_rec_devs <- log(extended_dec_vec / recent_mean) - (sigma_r^2)/2

for(i in 1:200) {
  # Add random variation to recruitment deviations
  random_devs <- rnorm(length(total_multiplier), mean = 0, sd = .15) #sd smaller than sigmaR to make the decline clear
  poor_recdevs[, i] <- base_rec_devs + random_devs
  
}

new_recdevs <- as.matrix(sq_recdevs[1:75, 4:203])

full_poor_recdevs <- rbind(new_recdevs, poor_recdevs)
matplot(y = full_poor_recdevs, type = "l")
matplot(y = poor_recdevs, type = "l")
abline(h = 0)
abline(v = 75)

# save(list = "full_poor_recdevs", file = file.path(main.dir, "Inputs", "poor_recdevs_mat.RData"))
```

## Simulating F trajectory
We simulated 200 stochastic F time series, following the same general pattern of the historical time series of F for the commercial and non commercial fleets estimated in the original Opakapaka assessment model. For the 25 years in the future, we used an average F value from the terminal 5 years. F values also had fairly strong temporal autocorrelation (0.7) which makes sense because management is consistent over 3-5 years of time and you would not expect fishing pressure to change drastically from year to year randomly. For non-commercial F, after year 2004, we calculated it as a ratio of 1.03 to commercial F, as is done in the most recent assessment. 
```{r}

#simple_F.csv was created by taking the original F time series and simplifying it so that the trend matched the general shape (up, constant, down, constant) but was simplified so that the values were means of each period of time.
F_df <- read.csv("Inputs/simple_F.csv")

# Get the mean F of last 5 years from Opaka assessment
mean_F <- F_df %>% 
  slice((n()-4):n()) %>%
  summarise(mean_FRS = mean(FRS), 
            mean_Non_comm = mean(Noncomm))

FRS_mu_ts <- c(F_df$FRS[-1], rep(mean_F$mean_FRS, 25))
NC_mu_tcs <- c(F_df$Noncomm[-1], rep(mean_F$mean_Non_comm, 25))

n_sims <- 200
temporal_cor <- 0.7
# Function to calculate temporally correlated F values based on a time series of "true" F values 
calculate_F <- function(n_sims, n_years, mu_ts, temporal_cor){
  F_df <- matrix(data = NA, nrow = n_years, ncol = n_sims)
  for(sim in 1:n_sims) {
      # Initialize first year with bounds
      F_df[1, sim] <- pmax(0.001, pmin(0.08, rnorm(1, mean = mu_ts[1], sd = 0.002)))
      
      # Generate subsequent years with AR(1) correlation
      for(year in 2:n_years) {
        # AR(1): F_t = rho * (F_{t-1} - mu_{t-1}) + mu_t + epsilon_t
        temp_cor <- rnorm(1, 0, 0.002 * sqrt(1 - temporal_cor^2))
        F_candidate <- temporal_cor * (F_df[year-1, sim] - mu_ts[year-1]) + 
                                mu_ts[year] + temp_cor
        F_df[year, sim] <- pmax(0.001, pmin(0.08, F_candidate))
      }
  }
  return(F_df)
}

F_comm_df <- calculate_F(n_sims = 200, n_years = 100, mu_ts = FRS_mu_ts, temporal_cor)
matplot(y = F_comm_df, type = "l", lty = 1, lwd = 1.2)

F_noncomm_df <- calculate_F(n_sims = 200, n_years = 55, mu_ts = NC_mu_tcs[1:75], temporal_cor)
F_noncomm_df <- rbind(F_noncomm_df, (F_comm_df[56:100,]*1.03))
matplot(y = F_noncomm_df, type = "l", lty = 1, lwd = 1.2)

plot(x = 1:100, y = F_comm_df[,1], type = "l")
lines(x = 1:100, y = F_noncomm_df[,1], col = "red")

# Save output 
# save(list = c("F_noncomm_df", "F_comm_df"), file = file.path(main.dir, "Inputs", "constantF_mat.RData"))

```

### Get effective sample sizes for FRS length data 

```{r}
#| eval: false

#effN is used for length comp data for FRS
effN <- rep$sizedbase %>% filter(Fleet == 1) %>% group_by(Yr) %>% distinct(effN) 
write.csv(effN, file.path(main.dir, "Inputs", "effN.csv"))

```


### Load already created RecDev and F vectors

```{r}
load(file.path(main.dir, "Inputs", "recdevs_mat.RData"))
load(file.path(main.dir, "Inputs", "poor_recdevs_mat.RData"))
load(file.path(main.dir, "Inputs", "constantF_mat.RData"))
```

### Create the directories for OSG and target_dir.txt

```{r}

library(fs)

scen <- "HRF_poorrec"
nyears_fwd <- 25
niter <- 100
scen_dirs <- file.path(main.dir, paste(scen, nyears_fwd, "yrfwd", sep = "_"), 1:niter)
sapply(scen_dirs, dir.create, recursive = T)
# file.copy(from = file.path(main.dir, "Inputs", "recdevs_mat.RData"), file.path(scen_dirs, "recdevs_mat.RData"))
file.copy(from = file.path(main.dir, "Inputs", "poor_recdevs_mat.RData"), file.path(scen_dirs, "poor_recdevs_mat.RData"))
file.copy(from = file.path(main.dir, "Inputs", "setseed.csv"), file.path(scen_dirs, "setseed.csv"))
file.copy(from = file.path(main.dir, "Inputs", "sas.csv"), file.path(scen_dirs, "sas.csv"))
file.copy(from = file.path(main.dir, "Inputs", "constantF_mat.RData"), file.path(scen_dirs, "constantF_mat.RData"))
file.copy(from = file.path(main.dir, "Inputs", "effN.csv"), file.path(scen_dirs, "effN.csv"))
# file.copy(from = file.path(main.dir, "Inputs", "simple_F.csv"), file.path(scen_dirs, "simple_F.csv"))

for(i in 1:length(scen_dirs)){
    fs::dir_copy(file.path(main.dir, "models", paste0("opaka-om-", nyears_fwd, "-r0trend")), scen_dirs[i]) #"-FRS-only" #, "-selex" , "-R0_trend", "_FRSlencomp"
    fs::dir_copy(file.path(main.dir, "models", paste0("opaka-em-", nyears_fwd, "-r0trend")), scen_dirs[i])
}
# only for FRS_only_poorrec_25_yrfwd scenario (testing hyperstablity)
# for(i in 1:length(scen_dirs)){
#     file.copy(from = file.path(main.dir, "FRS_only_25_yrfwd", i, "em", "ss3.dat"), 
#     to = file.path(scen_dirs[i], "normal_rec.dat"))
# }

for(i in 1:length(scen_dirs)){
    
    shell(paste0("powershell cd ", scen_dirs[i], ";tar -czf start.tar.gz ./*"))
    files_clean <- list.files(scen_dirs[i], pattern = ".csv|.RData|opaka-|normal_rec.dat", full.names = T)
    unlink(files_clean, recursive = T)
}

target_dirs <- paste0(paste(scen, nyears_fwd, "yrfwd", sep = "_"), "/",  1:niter)
writeLines(target_dirs, file.path(main.dir, paste(scen, nyears_fwd, "yrfwd", sep = "_"), "target_dirs.txt"))
shell(paste0("powershell cd ", main.dir, ";tar -czf upload.", paste(scen, nyears_fwd, "yrfwd", sep = "_"), ".tar.gz ", paste(scen, nyears_fwd, "yrfwd", sep = "_")))
#upload these files to OSG 
```

### Getting results

```{r}
#get results out of models for comparisons
all_scenario_names <- paste(sas_full$Scen_name, sas_full$N_years, "yrfwd", sep = "_")[c(1,3,5,7,8)]

missing_vec <- c()
for(i in 1:length(all_scenario_names)){
    if(file.exists(file.path(main.dir, paste0("download.", all_scenario_names[i], ".tar.gz ")))){
            shell(paste0("powershell cd ", main.dir, ";tar -xzf download.", all_scenario_names[i], ".tar.gz ", all_scenario_names[i]))
            for(y in 1:niter){
                if(file.exists(file.path(main.dir, all_scenario_names[i], y, "End.tar.gz"))){
                    shell(paste0("powershell cd ", main.dir, "/", all_scenario_names[i], "/", y, ";tar -xzf End.tar.gz"))
                }else{
                    missing_vec <- c(missing_vec, paste0(all_scenario_names[i], y))
                } 
            }
    }else{
        next
    }

}

get_results_all(
    overwrite_files = T,
    user_scenarios =  all_scenario_names[i],
    filename_prefix = "hrf_poorrec_r0_blk"
)   
get_fits_all(
    overwrite_files = T,
    user_scenarios = all_scenario_names,
    filename_prefix = "all_SQ"
)

```

### Checking results

```{r}

reps <- SSgetoutput(dirvec = c(
# file.path(main.dir, "HRF_poorrec", "11", "om"),
# file.path(main.dir, "HRF_poorrec", "11", "em"),
file.path(main.dir, "HRF_poorrec_25_yrfwd", "3", "om"),
file.path(main.dir, "HRF_poorrec_25_yrfwd", "3", "em"),
file.path(main.dir, "HRF_SQ_25_yrfwd", "3", "om"),
file.path(main.dir, "HRF_SQ_25_yrfwd", "3", "em")
))

reps_sum <- SSsummarize(reps)
#reps_sum <- SSsummarize(reps)
SSplotComparisons(reps_sum)
SSplotComparisons(reps_sum, col = c(1,2), models = c(1,2), uncertainty = F)
SSplotComparisons(reps_sum, legendlabels = c("prOM", "prEM"),  models = c(3,4), uncertainty = F)
SS_plots(reps$replist2)


reps_sum$recruits
apply(reps_sum$recruits[87:102,1:4], 2, mean)

ts_re %>%
filter(year <= 2048) %>%
group_by(scenario, year) %>% 
summarise(med_re = mean(SpawnBio_re),
uci = quantile(SpawnBio_re, .975),
lci = quantile(SpawnBio_re, .05)) %>% 
separate(col = scenario, into = c("scen", "Rec_scenario", "25", "Yrs"), sep = "_",
remove = F) %>% 
ggplot() +
geom_hline(yintercept = 0, color = "grey60") +
geom_ribbon(aes(x = year, ymin = lci, ymax = uci, fill = scen), alpha = .25) + 
geom_line(aes(x = year, y = med_re, color = scen), linewidth = 1.2) +
facet_wrap(~ Rec_scenario) + 
labs(x = "Year", y = "SSB MRE") + 
theme_bw()


good_its <- ts_re %>% filter(year == 2000 & SpawnBio_re < .05 & SpawnBio_re > -0.5) %>% select(iteration, scenario) %>% filter(scenario == "HRF_poorrec_25_yrfwd") %>% pull(iteration)

ts_re %>%
filter(iteration %in% good_its) %>% 
# group_by(scenario, year) %>%
# summarise(mean = mean(SpawnBio_re)) %>%
ggplot() +
geom_line(aes(x = year, y = SpawnBio_re, group = iteration, color = iteration)) +
geom_hline(yintercept = 0) +
facet_wrap(~scenario)

ts_re %>% filter(year == 1965 & SpawnBio_re > .1) %>% select(iteration, scenario)



```

Questions from Punt et al. 2021 to think about for this project: 1. How does the ability to estimate quantities of management interest change with average annual (effective) sample sizes for length-comp and conditional age-length data? 2. Does including data sources with small sample sizes in an assessment lead to bias? 3. What are the implications of collecting length and age samples at intervals greater than one year but large sample sizes? 4. How does the assessment performance change if sampling is unbalanced over time and space? Questions from Ono et al. 2015 1. What is the value of age vs length composition data? 2. What is the influence of age composition collection frequency and duration? 3. What is the influence of frequency and duration of survey composition data? 4. What is the impact of historical fishery composition data? 5. What is the impact of composition data sample size? 6. Which fishing pattern produces the most informative data?

### Run likelihood profiles

To profile over an F value for a given year, use the following code:

```{r}
em_I <- "SQ_0_yrfwd"
Iteration <- 5
nyears <- 75
like_dir <- file.path(main.dir, em_I, Iteration, "em", "likelihood_prof")
dir.create(like_dir)
files <- c("ss3.dat", "control.ss_new", "starter.ss", "forecast.ss")
file.copy(from = file.path(main.dir, em_I, Iteration,"em", files), to = like_dir, overwrite = T)
file.copy(from = file.path(main.dir, "bin", "ss3_win.exe"), to = like_dir, overwrite = T)
## Create fvec of values to profile over
rep_tmp <- SS_output(file.path(main.dir, em_I, Iteration, "em"), verbose = F)
fest <- rep_tmp$exploitation %>% filter(Yr == nyears) %>% pull(Comm)
fvec <- seq(fest - 0.01, fest + 0.02, by = 0.001)
fdf <- rep_tmp$exploitation %>% 
select(Yr, Seas, Comm, Non_comm) %>%
pivot_longer(cols = c("Comm", "Non_comm")) %>% 
mutate(Fleet = recode(name, Comm = "1", Non_comm = "2"),
Fleet = as.numeric(Fleet),
se = 0.5,
phase = -1) %>%
filter(Yr > 0) %>%
arrange(Fleet, Yr) %>% 
select(Fleet, Yr, Seas, value, se, phase) 

## Fix selex first 
CTL <- SS_readctl_3.30(file.path(like_dir, "control.ss_new"), datlist = file.path(like_dir, "ss3.dat"))
CTL$size_selex_parms$PHASE <- abs(CTL$size_selex_parms$PHASE) * -1
SS_writectl_3.30(CTL, outfile = file.path(like_dir, "control.ss_new"), overwrite = T)
rm(CTL)
## read in ctl file
CTL <- readLines(file.path(like_dir, "control.ss_new"))
# selex_lines <- grep("#_SizeSelex", CTL)
# end_selex_lines <- grep("#_AgeSelex", CTL)
# selex_params <- CTL[(selex_lines+2):(end_selex_lines-1)]
# selex <- strsplit(selex_params, " ")
# selex_new <- list()
# for(i in 1:length(selex)){
#     ind <- which(selex[[i]] == 3)
#     if(length(ind) == 1){
#         selex[[i]][ind] <- "-3"
#     }
#     selex[[i]] <- selex[[i]][-which(selex[[i]] == "")]
#     selex_new[[i]] <- str_flatten(selex[[i]], collapse =  " ")
# }
# ## replace selex values
# CTL[237] <- selex_new[[1]]
# CTL[238] <- selex_new[[2]]
# CTL[239] <- selex_new[[3]]
# CTL[240] <- selex_new[[4]]
# CTL[241] <- selex_new[[5]]
# CTL[242] <- selex_new[[6]]

## find lines where to break the file and insert new lines
start_ind <- grep("#Fishing Mortality info", CTL)
end_ind <- grep("#_initial_F_parms", CTL)

f_setup <- paste0("## add new fishing mortality info \n",
"#Fishing Mortality info \n",
"0.3 # F ballpark value in units of annual_F \n",
"-2001 # F ballpark year (neg value to disable) \n",
"4 # F_Method:  1=Pope midseason rate; 2=F as parameter; 3=F as hybrid; 4=fleet-specific parm/hybrid (#4 is superset of #2 and #3 and is recommended) \n",
"2.9 # max F (methods 2-4) or harvest fraction (method 1) \n",
"#fleet parameter_value phase \n",
"1 0.015 1 \n",
"2 0.015 1 \n",
"-9998 1 1 \n",
"3 #Ntuning loops \n",
"# detailed setup for F_Method=2; -Yr to fill remaining years \n",
"#Fleet Yr Seas F_value se phase \n")

f_detailed_ender <- " -9999 1 1 1 1 1 \n"


## write out new ctl file
writeLines(CTL[1:(start_ind -1)], file.path(like_dir, "control_modified.ss"))
CON <- file(file.path(like_dir, "control_modified.ss"), "a")
cat(f_setup, file = CON)
write.table(fdf, file = CON, append = T, col.names = F, row.names = F)
cat(f_detailed_ender, file = CON) 
writeLines(CTL[end_ind:length(CTL)], CON)
close(CON)

#adjust se for catch 
dat <- SS_readdat_3.30(file = file.path(like_dir, "ss3.dat"), verbose = F)
dat$catch$catch_se <- .5
SS_writedat(dat, outfile = file.path(like_dir, "ss3.dat"), overwrite = T)
## Create profile directories
profile_dirs <- paste0(like_dir, "/profile_", seq(1,length(fvec)))
sapply(profile_dirs, dir.create)
profile.files <- c("ss3.dat", "control_modified.ss", "starter.ss", "forecast.ss", "ss3_win.exe")
for (i in 1:length(fvec)){
    file.copy(from = file.path(like_dir, profile.files), to = profile_dirs[i], overwrite = T)

    CTL <- readLines(file.path(profile_dirs[i], "control_modified.ss"))
    line_ind <- grep(paste0("1 ",nyears, " 1"), CTL)
    CTL[line_ind] <- paste0("1 ", nyears, " 1 ", fvec[i], " 0.5 -1")
    writeLines(CTL, file.path(profile_dirs[i], "control_modified.ss"))

    START <- SS_readstarter(file.path(profile_dirs[i], "starter.ss"))
    START$ctlfile <- "control_modified.ss"
    START$prior_like <- 1
    START$init_values_src <- 0
    SS_writestarter(START, profile_dirs[i], overwrite = T)

    run(dir = profile_dirs[i], exe = "ss3_win.exe", extras = "-nohess", skipfinished = F, verbose = F)

}



```

```{r}
#em_I <- paste0("em_", n_yrs_fwd)
em_I <- "HRFhyperstable_15_yrfwd"
Iteration <- 1
like_dir <- file.path(main.dir, em_I, Iteration, "em", "likelihood_prof")
dir.create(like_dir)
files <- c("ss3.dat", "control.ss_new", "starter.ss", "forecast.ss", "ss.par", "ss3.exe")
file.copy(from = file.path(main.dir, em_I, Iteration,"em", files), to = like_dir, overwrite = T)
CTL <- SS_readctl_3.30(file = file.path(like_dir, "control.ss_new"), datlist = file.path(like_dir, "ss3.dat"))
CTL <- SS_readctl_3.30(file = file.path(main.dir, em_I, Iteration, "em", "control.ss_new"), datlist = file.path(main.dir, em_I, Iteration, "em", "ss3.dat"))
# r0 <- CTL$SR_parms$INIT[1]
# r0_vec <- seq(r0 - 1, r0 + 1, by = 0.2)
CTL$F_setup2

par.file <- SS_readpar_3.30(parfile = file.path(like_dir, "ss.par"), datsource = file.path(like_dir, "ss3.dat"), ctlsource = file.path(like_dir, "control.ss_new"))
F_term <- par.file$F_rate %>% filter(fleet == 1 & year == nyears) %>% pull(F)
F_term_vec <- seq(F_term - .01, F_term + 0.1, by = 0.005)

START <- SS_readstarter(file = file.path(like_dir, "starter.ss"), verbose = FALSE)
START$prior_like <- 1
START$ctlfile <- "control_modified.ss"
START$init_values_src <- 1
SS_writestarter(START, dir = like_dir, overwrite = TRUE, verbose = F)

likes <- profile_F(
  dir = like_dir,
  newctlfile = "control_modified.ss",
  linenum = nyears,
  #string = "Fvalue",
  profilevec = F_term_vec[10:20],
  usepar = T,
  parstring = "# F_rate[90]:",
  exe = "ss3",
  verbose = FALSE,
  extras="-nohess -nox"
)



likes <- profile(
  dir = like_dir,
  newctlfile = "control_modified.ss",
  linenum = 241,
  #string = "Fvalue",
  profilevec = F_term_vec,
  usepar = T,
  parstring = "# F_rate[90]:",
  exe = "ss3",
  verbose = FALSE,
  extras="-nohess -nox"
)

profile_mods <- SSgetoutput(dirvec = like_dir, keyvec = c(1:5,7,10))
profile_mods_sum <- SSsummarize(profile_mods)
profile_mods_sum$maxgrad <= 1e-4

SSplotProfile(profile_mods_sum,
  profile.string = "SR_LN",
  profile.label = "SR_LN(R0)",
  print = TRUE,
  plotdir = like_dir,

)
PinerPlot(profile_mods_sum, component = "Length_like", print = TRUE, plotdir = like_dir)
PinerPlot(profile_mods_sum, component = "Surv_like")




```