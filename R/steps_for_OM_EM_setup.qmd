---
title: "Creating the OMs"
format: html
---

```{r}
library(r4ss)
library(tidyverse)
library(ss3sim)
library(stringr)
main.dir <- this.path::here(.. = 1)
source(file.path(main.dir, "R", "simF.r"))
source(file.path(main.dir, "R", "make_dummy_dat.r"))
source(file.path(main.dir, "R", "get_fits.r"))
set.seed <- read.csv(file.path(main.dir, "Inputs", "setseed.csv"))
sas_full <- read.csv(file.path(main.dir, "Inputs", "sas.csv"))
niter <- 100
rep <- SS_output(dir = file.path(main.dir, "models", "03-simplified-mod"))
```

### Create rec dev timeseries

I generated a time series of recruitment deviations that result in a 40% decline in average recruitment. To do this, I first calculated the expected recruitment based on the stock-recruitment curve for the first 75 years (solid yellow line). I found the average of that (dotted yellow line) and what a 40% decrease would be. From there, I created a vector of the rate of decline over 10 years to reach the 40% reduced level and then the reduced multiplier. I iteratively tested to find the true value of the decline multiplier that leads to a 40% decline in mean expected recruitment.

```{r}
recdevs_df <- rep$parameters %>% 
filter(str_detect(Label, "Rec")) %>% 
select(c(Label, Value)) %>% 
separate(col = Label, sep = "_", into = c("period", "recdev", "year")) 

ssb <- rep$derived_quants %>% 
filter(str_detect(Label, "SSB")) %>%
separate(col = Label, into = c("SSB", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year)) %>%
filter(!is.na(Year)) %>% 
pull(Value)

rec <- rep$derived_quants %>% 
filter(str_detect(Label, "Rec")) %>% 
separate(col = Label, into = c("Rec", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year)) %>%
filter(!is.na(Year)) %>% 
select(Year, Value) %>% 
rename("Rec" = "Value") %>% 
mutate("SSB" = ssb, 
"SR_rec" = (SSB*4*.76*180.859)/(1466.7*(1-.76)+SSB*(5*.76-1)),
"Calc_rec" = SR_rec*exp(c(0,recdevs_df$Value) - .657^2/2), 
"Rec_dev" = c(0, recdevs_df$Value))

#getting recruitment down to 40% reduction level
# 253.55 = last year of recruitment from the model ("rec$Rec")
#166.7759 is mean rec from model for last 5 years
# dec_rec <- seq(166.7759, 146.089, length.out = 11) 

# dec_rec_2 <- rnorm(n = 15, mean = 146.089, sd = 25)
# dec_rec_full <- c(dec_rec[-1], dec_rec_2)
# test_recdevs <- log(dec_rec_full/167.6124)
# test_recdevs_mat <- matrix(data = c(0,recdevs_df$Value,test_recdevs), ncol = 1)

# log_devs_dec <- log(dec_rec[-1]/167.6124) + ((0.15^2)/2)
# log_devs_phase2 <- log(rep(100,15)/180.859) + ((0.15^2)/2)

decline_rate <- (0.65^(1/10)) - 1  # -5.1% per year for 40% total decline
years_decline <- 10
years_constant <- 15

# Create multiplier vector for decline years
decline_multiplier <- numeric(years_decline)
for(t in 1:years_decline) {
  decline_multiplier[t] <- (1 + decline_rate)^t
}

# Step 2: Create constant multiplier for remaining years
constant_multiplier <- rep(0.65, years_constant)  # 40% decline = 60% of original

# Step 3: Combine both periods
total_multiplier <- c(decline_multiplier, constant_multiplier)

# Step 4: Apply to your recent mean
recent_mean <- 162.4478  #166.7759mean from last 5 years #162.4478 is mean SR-curve rec
extended_dec_vec <- recent_mean * total_multiplier

# Step 5: Calculate recruitment deviations for all 25 years
sigma_r <- .35
extended_rec_devs <- log(extended_dec_vec / recent_mean) - ((sigma_r^2)/2)
plot(x = 1:length(extended_rec_devs), y = extended_rec_devs)

omctl <- SS_readctl_3.30(file = file.path("models", "opaka-om-25-recdevs-test", "codOM.ctl"),
datlist = file.path("models", "opaka-om-25-recdevs-test", "codOM.dat"))
omctl$recdev_input[76:100,2] <- extended_rec_devs
omctl$F_setup2[76:100,"Fvalue"] <- F_comm_df[76:100,1]
omctl$F_setup2[176:200,"Fvalue"] <- F_noncomm_df[76:100,1]
# omctl$F_setup2[76:100,"Fvalue"] <- 0
# omctl$F_setup2[176:200,"Fvalue"] <- 0

SS_writectl_3.30(omctl, outfile = file.path("models", "opaka-om-25-recdevs-test", "codOM.ctl"), overwrite = T)

#run ss3 -nohess for om

reptest <- SS_output(file.path("models", "opaka-om-25-recdevs-test"))

reptest$derived_quants %>% filter(str_detect(Label, "Rec")) %>%
separate(col = Label, into = c("Recr", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year),
    period = ifelse(Year <=75 & Year >=69, "early", "late"),
    period = ifelse(Year <69, "NA", period),
    period = ifelse(Year >75 & Year <=85, "decline", period),
    period = ifelse(is.na(Year), "init", period)) %>%
    group_by(period) %>% summarise(mean = mean(Value), min = min(Value), max = max(Value))


reptest_ssb <- reptest$derived_quants %>%
filter(str_detect(Label, "SSB")) %>% 
separate(col = Label, into = c("SSB", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year)) %>%
filter(!is.na(Year)) %>%
mutate(
    period = ifelse(Year <=75 & Year >=69, "early", "late"),
    period = ifelse(Year <69, "NA", period),
    period = ifelse(Year >75 & Year <=85, "decline", period),
    period = ifelse(is.na(Year), "init", period)) %>%
    group_by(period) %>% summarise(mean = mean(Value), min = min(Value), max = max(Value))

reptest$parameters %>% filter(str_detect(Label, "RecrDev")) %>% select(c(Label, Value))



reptest_rec %>% ggplot(aes(x = Year, y = Value)) + geom_line() + geom_point()
reptest_ssb %>% ggplot(aes(x = Year, y = Value)) + geom_line()

plot(rec$Year, y = rec$SR_rec, type = "l", col = "orange", xlim = c(1,100), ylim = c(0,800))
abline(h = 187.0477, col = "darkblue", lty = 2) #mean Rec
abline(h = 167.6124, col = "darkorange", lty = 2) #mean SR recruitment
abline(h = 100.5674, col = "red4", lty = 2) #reduced by 40% mean SR recruitment
lines(rec$Year, y = rec$Rec, col = "blue")
lines(rec$Year, y = rec$Calc_rec, col = "blue", lty = 2)
lines(76:100, y = extended_dec_vec, col = "red", lty = 3, lwd = 1.5)
#lines(86:100, y = extended_dec_vec, col = "red", lty = 3, lwd = 1.5)
lines(1:100, y = reptest_rec$Value, col = "red")

```

#### Rec dev matrix

Create 100 rec dev trajectories for 75 years, with added variability (sigmaR = .15) in the last 10 years. The first 65 years will follow the recruitment deviations that came out of the assessment model to retain any sort of environmental signal it may have picked up on.

```{r}
plot(x = recdevs_df$year, y = recdevs_df$Value, type = "l")
sigmar <- .15 #.657 from opaka model
new_recdevs <- matrix(data = NA, nrow = nrow(recdevs_df)+1, ncol = 100)
recdevs <- matrix(data = NA, nrow = nrow(recdevs_df)+1, ncol = 100)
new_recdevs[1,] <- 0
for(i in 2:nrow(new_recdevs)){
    if(i < 60){
        recdev_sd <- .01
        new_recdevs[i,] <- faux::rnorm_multi(n = 100, ## year vector
                                 vars = 1,
                                 mu = recdevs_df$Value[i-1],
                                 r = 1-1e-5, #correlation
                                 sd = recdev_sd)[,1]
    }else{
        recdevs[i,] <- rnorm(n = 100, mean = 0, sd = 1)  
        new_recdevs[i,] <- sigmar * recdevs[i,] - sigmar^2 / 2 
    }


}
matplot(new_recdevs, type = "l", ylim = c(-1,2))
```

Then we add 25 years of assumed "normal" future recruitment, with the same variaiblity as the last 10 years of the historical recdevs (sigmaR = 0.15).

```{r}
future_recdevs <- matrix(data = NA, nrow = 50, ncol = 100)
for(i in 1:nrow(future_recdevs)){ 
    recdevs <- rnorm(n = 100, mean = 0, sd = 1)   
    future_recdevs[i,] <- sigmar * recdevs - sigmar^2 / 2 
}
matplot(y = future_recdevs, type = "l")

full_recdevs <- rbind(new_recdevs, future_recdevs)
matplot(y = full_recdevs, type = "l")
#save(list = "full_recdevs", file = file.path(main.dir, "Inputs", "recdevs_mat.RData"))
```

We also create 25 years of "poor" future recruitment. Using the full_recdevs, 100 OM and EMs were generated and the results were aggregated using the `get_results_all` function for that scenario (read in as `lrf` in code chunk below). The recruitment in year 75 for each iteration (should be the same across scenarios for the same iteration) was taken and then a trajectory was created that brought that value down to the calculated 40% reduced mean recruitment. Those values were randomly sampled with a sd of `sigmaR` to add noise and those recdevs were added to the `new_recdevs` to create a second matrix, with poor recruitment values.  

```{r}

## years of poor recruitment in the future
poor_recdevs <- matrix(data = NA, nrow = 25, ncol = 100)
lrf <- read.csv(file.path(main.dir, "LRF_SQ_ts.csv"))
rec75 <- lrf %>% 
filter(model_run == "om" & year == 75) %>% 
pull(Recruit_0)
sigmar <- .15
for(i in 1:ncol(new_recdevs)){
    
    dec_rec <- seq(rec75[i], 100.5675, length.out = 11) #getting recruitment down to 40% reduction level of mean recruitment
    dec_rec_full <- c(dec_rec[-1], rep(100.5675, 15))
    dec_recdevs <- log(dec_rec_full/167.6124)
    poor_recdevs[,i] <- rnorm(25, mean = dec_recdevs, sd = .15)

}
full_poor_recdevs <- rbind(new_recdevs, poor_recdevs)
matplot(y = full_poor_recdevs, type = "l")
matplot(y = full_recdevs, type = "l", add = T)
abline(h = 0)
abline(v = 75)

# save(list = "full_poor_recdevs", file = file.path(main.dir, "Inputs", "poor_recdevs_mat.RData"))
```

## Simulating F trajectory

```{r}
for(i in 1:nrow(future_cf)){ 
    future_cf[i,] <- faux::rnorm_multi(n = 100, ## year vector
                                vars = 1,
                                mu = 0.0325262, # terminal year F from model
                                r = 1-1e-5, #correlation
                                sd = 0.0075)[,1]
}
matplot(y = future_cf, type = "l", lty = 1, lwd = 1.2)
F_comm_mod <- rep$exploitation$Comm[-1]
F_comm_df <- matrix(data = rep(F_comm_mod, 100), nrow = 75, ncol = 100,byrow = F)
F_comm_df <- rbind(F_comm_df, future_cf)
F_noncomm_mod <- rep$exploitation$Non_comm[-1]
F_noncomm_df <- matrix(data = rep(F_noncomm_mod, 100), nrow = 75, ncol = 100,byrow = F)
F_noncomm_df <- rbind(F_noncomm_df, future_cf * 1.03)
# save(list = c("F_noncomm_df", "F_comm_df"), file = file.path(main.dir, "Inputs", "constantF_mat.RData"))

## Increasing F
F_up_mu <- seq(F_comm_mod[75], 0.0491544, length.out = 20) #increase back up to 75 percentile of historical Fs
future_F_up <- matrix(data = NA, nrow = 50, ncol = 100)
for(i in 1:nrow(future_F_up)){
    if(i <=20){
         future_F_up[i, ] <- faux::rnorm_multi(n = 100, ## year vector
                                vars = 1,
                                mu = F_up_mu[i], # increasing F trajectory
                                r = 1-1e-5, #correlation
                                sd = 0.0015)[,1]
    }else{
        future_F_up[i, ] <- faux::rnorm_multi(n = 100, ## year vector
                            vars = 1,
                            mu = F_up_mu[20], # stay around the last year of increasing 
                            r = 1-1e-5, #correlation
                            sd = 0.0015)[,1]
    }

}
matplot(y = future_F_up, type = "l", lty = 1, lwd = 1.2)
F_comm_up <- rbind(F_comm_df, future_F_up)
F_noncomm_up <- rbind(F_noncomm_df, future_F_up * 1.03)
# save(list = c("F_noncomm_up", "F_comm_up"), file = file.path(main.dir, "Inputs", "increaseF_mat.RData"))
```

### Load already created RecDev-vectors and get F from simplified model

```{r}
F_comm_mod <- rep$exploitation$Comm[-1]
F_noncomm_mod <- rep$exploitation$Non_comm[-1]
load(file.path(main.dir, "Inputs", "recdevs_mat.RData"))
load(file.path(main.dir, "Inputs", "poor_recdevs_mat.RData"))
load(file.path(main.dir, "Inputs", "constantF_mat.RData"))
load(file.path(main.dir, "Inputs", "increaseF_mat.RData"))
```

### Create the directories for OSG and target_dir.txt

```{r}

library(fs)

scen <- "LRF_SQ"
nyears_fwd <- 25
niter <- 100
scen_dirs <- file.path(main.dir, paste(scen, nyears_fwd, "yrfwd", sep = "_"), 1:niter)
sapply(scen_dirs, dir.create, recursive = T)
file.copy(from = file.path(main.dir, "Inputs", "recdevs_mat.RData"), file.path(scen_dirs, "recdevs_mat.RData"))
#file.copy(from = file.path(main.dir, "Inputs", "poor_recdevs_mat.RData"), file.path(scen_dirs, "poor_recdevs_mat.RData"))
file.copy(from = file.path(main.dir, "Inputs", "setseed.csv"), file.path(scen_dirs, "setseed.csv"))
file.copy(from = file.path(main.dir, "Inputs", "sas.csv"), file.path(scen_dirs, "sas.csv"))
file.copy(from = file.path(main.dir, "Inputs", "constantF_mat.RData"), file.path(scen_dirs, "constantF_mat.RData"))

for(i in 1:length(scen_dirs)){
    fs::dir_copy(file.path(main.dir, "models", paste0("opaka-om-", nyears_fwd)), scen_dirs[i]) #"-FRS-only" #, "-selex" , "-R0_trend"
    fs::dir_copy(file.path(main.dir, "models", paste0("opaka-em-", nyears_fwd)), scen_dirs[i])
}
#only for FRS_only_poorrec_25_yrfwd scenario (testing hyperstablity)
# for(i in 1:length(scen_dirs)){
#     file.copy(from = file.path(main.dir, "FRS_only_25_yrfwd", i, "em", "ss3.dat"), 
#     to = file.path(scen_dirs[i], "normal_rec.dat"))
# }

for(i in 1:length(scen_dirs)){
    
    shell(paste0("powershell cd ", scen_dirs[i], ";tar -czf start.tar.gz ./*"))
    files_clean <- list.files(scen_dirs[i], pattern = ".csv|.RData|opaka-|normal_rec.dat", full.names = T)
    unlink(files_clean, recursive = T)
}

target_dirs <- paste0(paste(scen, nyears_fwd, "yrfwd", sep = "_"), "/", 1:niter)
writeLines(target_dirs, file.path(main.dir, paste(scen, nyears_fwd, "yrfwd", sep = "_"), "target_dirs.txt"))
shell(paste0("powershell cd ", main.dir, ";tar -czf upload.", scen, "_25_yrsfwd.tar.gz ", scen, "_25_yrfwd/"))
#upload these files to OSG 
```

### Getting results

```{r}
#get results out of models for comparisons
all_scenario_names <- paste(sas_full$Scen_name, sas_full$N_years, "yrfwd", sep = "_")[1:8]

missing_vec <- c()
for(i in 1:length(all_scenario_names)){
    if(file.exists(file.path(main.dir, paste0("download.", all_scenario_names[i], ".tar.gz ")))){
            shell(paste0("powershell cd ", main.dir, ";tar -xzf download.", all_scenario_names[i], ".tar.gz ", all_scenario_names[i]))
            for(y in 1:niter){
                if(file.exists(file.path(main.dir, all_scenario_names[i], y, "End.tar.gz"))){
                    shell(paste0("powershell cd ", main.dir, "/", all_scenario_names[i], "/", y, ";tar -xzf End.tar.gz"))
                }else{
                    missing_vec <- c(missing_vec, paste0(all_scenario_names[i], y))
                } 
            }
    }else{
        next
    }

}

get_results_all(
    overwrite_files = T,
    user_scenarios =  all_scenario_names,
    filename_prefix = "all_scen"
)   
get_fits_all(
    overwrite_files = T,
    user_scenarios = all_scenario_names,
    filename_prefix = "all_scen"
)

```

### Checking results

```{r}

reps <- SSgetoutput(dirvec = c(
file.path(main.dir, "models", "opaka-om-25-recdevs-test"),
file.path(main.dir, "models", "opaka-om-25-recdevs-test", "em")
# file.path(main.dir, "HRF_SQ_25_yrfwd", "10", "om"),
# file.path(main.dir, "HRF_SQ_25_yrfwd", "10", "em")
))
reps_sum <- SSsummarize(reps)
SSplotComparisons(reps_sum)
SSplotComparisons(reps_sum, legendlabels = c("prOM", "prEM"),  models = c(1,2), uncertainty = F)
SS_plots(reps$replist2)

reps$replist1$derived_quants %>% filter(str_detect(Label, "Rec")) %>%
separate(col = Label, into = c("Recr", "Year"), sep = "_") %>%
mutate(Year = as.numeric(Year),
    period = ifelse(Year <=75, "early", "late"),
    period = ifelse(Year >75 & Year <=85, "decline", period),
    period = ifelse(is.na(Year), "init", period)) %>%
    group_by(period) %>% summarise(mean = mean(Value), min = min(Value), max = max(Value))

rep <- SS_output(file.path(main.dir, "HRF_poorrec_perfectdata_25_yrfwd/9/em"))
SS_plots(rep)

ssb_msy <- rep$derived_quants %>% filter(str_detect(Label, "SSB_MSY")) %>% pull(Value) 
.865*ssb_msy
rep$exploitation

reps <- SSgetoutput(dirvec = c(
file.path(main.dir, "HRF_poorrec_perfectdata_25_yrfwd", "1", "om"),
file.path(main.dir, "HRF_poorrec_perfectdata_25_yrfwd", "1", "em"),
file.path(main.dir, "HRF_poorrec_perfectdata_25_yrfwd", "10", "om"),
file.path(main.dir, "HRF_poorrec_perfectdata_25_yrfwd", "10", "em")

))

reps <- SSgetoutput(dirvec = c(
file.path(main.dir, "HRF_SQ_25_yrfwd", "2", "om"),
file.path(main.dir, "LRF_SQ_25_yrfwd", "2", "om"),
file.path(main.dir, "HRF_test", "2_b", "om"),
file.path(main.dir, "HRF_poorrec_25_yrfwd", "2", "om")
))

reps_sum <- SSsummarize(reps)
#SSplotComparisons(reps_sum, legendlabels = c("HRF_om", "HRF_em", "LRF_om", "LRF_em"))  print = TRUE, png = TRUE, plotdir = file.path(main.dir, "HRF_test", "2")
SSplotComparisons(reps_sum, legendlabels = c("om1", "em1", "om10", "em10"))
SS_plots(reps$replist1)
SS_plots(reps$replist2)
SS_plots(reps$replist3)

reps_sum$maxgrad
reps$replist2$breakpoints_for_bias_adjustment_ramp
new_bias <- SS_fitbiasramp(reps$replist4)

reps_sum$indices %>% filter(Fleet == 3) %>% #filter(name == "replist1" | name == "replist2") %>%
mutate(name = recode(name, 
                    replist1 = "om_1", 
                    replist2 = "em_1", 
                    replist3 = "om_2",
                    replist4 = "em_2",
                    replist5 = "om_3",
                    replist6 = "em_3",
                    replist7 = "om_4",
                    replist8 = "em_4",
                    replist9 = "om_5",
                    replist10 = "em_5"
                    )) %>%
separate(name, sep = "_", into = c("mod_type", "rep")) %>%
ggplot(aes(x = Yr)) +
geom_point(data = . %>% filter(mod_type == "om"), aes(y = Exp, color = mod_type)) + 
geom_point(data = . %>% filter(mod_type == "em"), aes(y = Obs, color = mod_type)) + 
geom_line(data = . %>% filter(mod_type == "em"), aes(y = Exp, color = mod_type), linewidth = 1.5) +
facet_wrap(~rep) +
labs(x = "Year", y = "BFISH Index") + 
theme_classic() +
theme(legend.position = "top",
legend.text = element_text(size = 12))

```

Questions from Punt et al. 2021 to think about for this project: 1. How does the ability to estimate quantities of management interest change with average annual (effective) sample sizes for length-comp and conditional age-length data? 2. Does including data sources with small sample sizes in an assessment lead to bias? 3. What are the implications of collecting length and age samples at intervals greater than one year but large sample sizes? 4. How does the assessment performance change if sampling is unbalanced over time and space? Questions from Ono et al. 2015 1. What is the value of age vs length composition data? 2. What is the influence of age composition collection frequency and duration? 3. What is the influence of frequency and duration of survey composition data? 4. What is the impact of historical fishery composition data? 5. What is the impact of composition data sample size? 6. Which fishing pattern produces the most informative data?

### Run likelihood profiles

To profile over an F value for a given year, use the following code:

```{r}
em_I <- "SQ_0_yrfwd"
Iteration <- 5
nyears <- 75
like_dir <- file.path(main.dir, em_I, Iteration, "em", "likelihood_prof")
dir.create(like_dir)
files <- c("ss3.dat", "control.ss_new", "starter.ss", "forecast.ss")
file.copy(from = file.path(main.dir, em_I, Iteration,"em", files), to = like_dir, overwrite = T)
file.copy(from = file.path(main.dir, "bin", "ss3_win.exe"), to = like_dir, overwrite = T)
## Create fvec of values to profile over
rep_tmp <- SS_output(file.path(main.dir, em_I, Iteration, "em"), verbose = F)
fest <- rep_tmp$exploitation %>% filter(Yr == nyears) %>% pull(Comm)
fvec <- seq(fest - 0.01, fest + 0.02, by = 0.001)
fdf <- rep_tmp$exploitation %>% 
select(Yr, Seas, Comm, Non_comm) %>%
pivot_longer(cols = c("Comm", "Non_comm")) %>% 
mutate(Fleet = recode(name, Comm = "1", Non_comm = "2"),
Fleet = as.numeric(Fleet),
se = 0.5,
phase = -1) %>%
filter(Yr > 0) %>%
arrange(Fleet, Yr) %>% 
select(Fleet, Yr, Seas, value, se, phase) 

## Fix selex first 
CTL <- SS_readctl_3.30(file.path(like_dir, "control.ss_new"), datlist = file.path(like_dir, "ss3.dat"))
CTL$size_selex_parms$PHASE <- abs(CTL$size_selex_parms$PHASE) * -1
SS_writectl_3.30(CTL, outfile = file.path(like_dir, "control.ss_new"), overwrite = T)
rm(CTL)
## read in ctl file
CTL <- readLines(file.path(like_dir, "control.ss_new"))
# selex_lines <- grep("#_SizeSelex", CTL)
# end_selex_lines <- grep("#_AgeSelex", CTL)
# selex_params <- CTL[(selex_lines+2):(end_selex_lines-1)]
# selex <- strsplit(selex_params, " ")
# selex_new <- list()
# for(i in 1:length(selex)){
#     ind <- which(selex[[i]] == 3)
#     if(length(ind) == 1){
#         selex[[i]][ind] <- "-3"
#     }
#     selex[[i]] <- selex[[i]][-which(selex[[i]] == "")]
#     selex_new[[i]] <- str_flatten(selex[[i]], collapse =  " ")
# }
# ## replace selex values
# CTL[237] <- selex_new[[1]]
# CTL[238] <- selex_new[[2]]
# CTL[239] <- selex_new[[3]]
# CTL[240] <- selex_new[[4]]
# CTL[241] <- selex_new[[5]]
# CTL[242] <- selex_new[[6]]

## find lines where to break the file and insert new lines
start_ind <- grep("#Fishing Mortality info", CTL)
end_ind <- grep("#_initial_F_parms", CTL)

f_setup <- paste0("## add new fishing mortality info \n",
"#Fishing Mortality info \n",
"0.3 # F ballpark value in units of annual_F \n",
"-2001 # F ballpark year (neg value to disable) \n",
"4 # F_Method:  1=Pope midseason rate; 2=F as parameter; 3=F as hybrid; 4=fleet-specific parm/hybrid (#4 is superset of #2 and #3 and is recommended) \n",
"2.9 # max F (methods 2-4) or harvest fraction (method 1) \n",
"#fleet parameter_value phase \n",
"1 0.015 1 \n",
"2 0.015 1 \n",
"-9998 1 1 \n",
"3 #Ntuning loops \n",
"# detailed setup for F_Method=2; -Yr to fill remaining years \n",
"#Fleet Yr Seas F_value se phase \n")

f_detailed_ender <- " -9999 1 1 1 1 1 \n"


## write out new ctl file
writeLines(CTL[1:(start_ind -1)], file.path(like_dir, "control_modified.ss"))
CON <- file(file.path(like_dir, "control_modified.ss"), "a")
cat(f_setup, file = CON)
write.table(fdf, file = CON, append = T, col.names = F, row.names = F)
cat(f_detailed_ender, file = CON) 
writeLines(CTL[end_ind:length(CTL)], CON)
close(CON)

#adjust se for catch 
dat <- SS_readdat_3.30(file = file.path(like_dir, "ss3.dat"), verbose = F)
dat$catch$catch_se <- .5
SS_writedat(dat, outfile = file.path(like_dir, "ss3.dat"), overwrite = T)
## Create profile directories
profile_dirs <- paste0(like_dir, "/profile_", seq(1,length(fvec)))
sapply(profile_dirs, dir.create)
profile.files <- c("ss3.dat", "control_modified.ss", "starter.ss", "forecast.ss", "ss3_win.exe")
for (i in 1:length(fvec)){
    file.copy(from = file.path(like_dir, profile.files), to = profile_dirs[i], overwrite = T)

    CTL <- readLines(file.path(profile_dirs[i], "control_modified.ss"))
    line_ind <- grep(paste0("1 ",nyears, " 1"), CTL)
    CTL[line_ind] <- paste0("1 ", nyears, " 1 ", fvec[i], " 0.5 -1")
    writeLines(CTL, file.path(profile_dirs[i], "control_modified.ss"))

    START <- SS_readstarter(file.path(profile_dirs[i], "starter.ss"))
    START$ctlfile <- "control_modified.ss"
    START$prior_like <- 1
    START$init_values_src <- 0
    SS_writestarter(START, profile_dirs[i], overwrite = T)

    run(dir = profile_dirs[i], exe = "ss3_win.exe", extras = "-nohess", skipfinished = F, verbose = F)

}



```

```{r}
#em_I <- paste0("em_", n_yrs_fwd)
em_I <- "HRFhyperstable_15_yrfwd"
Iteration <- 1
like_dir <- file.path(main.dir, em_I, Iteration, "em", "likelihood_prof")
dir.create(like_dir)
files <- c("ss3.dat", "control.ss_new", "starter.ss", "forecast.ss", "ss.par", "ss3.exe")
file.copy(from = file.path(main.dir, em_I, Iteration,"em", files), to = like_dir, overwrite = T)
CTL <- SS_readctl_3.30(file = file.path(like_dir, "control.ss_new"), datlist = file.path(like_dir, "ss3.dat"))
CTL <- SS_readctl_3.30(file = file.path(main.dir, em_I, Iteration, "em", "control.ss_new"), datlist = file.path(main.dir, em_I, Iteration, "em", "ss3.dat"))
# r0 <- CTL$SR_parms$INIT[1]
# r0_vec <- seq(r0 - 1, r0 + 1, by = 0.2)
CTL$F_setup2

par.file <- SS_readpar_3.30(parfile = file.path(like_dir, "ss.par"), datsource = file.path(like_dir, "ss3.dat"), ctlsource = file.path(like_dir, "control.ss_new"))
F_term <- par.file$F_rate %>% filter(fleet == 1 & year == nyears) %>% pull(F)
F_term_vec <- seq(F_term - .01, F_term + 0.1, by = 0.005)

START <- SS_readstarter(file = file.path(like_dir, "starter.ss"), verbose = FALSE)
START$prior_like <- 1
START$ctlfile <- "control_modified.ss"
START$init_values_src <- 1
SS_writestarter(START, dir = like_dir, overwrite = TRUE, verbose = F)

likes <- profile_F(
  dir = like_dir,
  newctlfile = "control_modified.ss",
  linenum = nyears,
  #string = "Fvalue",
  profilevec = F_term_vec[10:20],
  usepar = T,
  parstring = "# F_rate[90]:",
  exe = "ss3",
  verbose = FALSE,
  extras="-nohess -nox"
)



likes <- profile(
  dir = like_dir,
  newctlfile = "control_modified.ss",
  linenum = 241,
  #string = "Fvalue",
  profilevec = F_term_vec,
  usepar = T,
  parstring = "# F_rate[90]:",
  exe = "ss3",
  verbose = FALSE,
  extras="-nohess -nox"
)

profile_mods <- SSgetoutput(dirvec = like_dir, keyvec = c(1:5,7,10))
profile_mods_sum <- SSsummarize(profile_mods)
profile_mods_sum$maxgrad <= 1e-4

SSplotProfile(profile_mods_sum,
  profile.string = "SR_LN",
  profile.label = "SR_LN(R0)",
  print = TRUE,
  plotdir = like_dir,

)
PinerPlot(profile_mods_sum, component = "Length_like", print = TRUE, plotdir = like_dir)
PinerPlot(profile_mods_sum, component = "Surv_like")




```